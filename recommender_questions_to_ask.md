# 0: Instructions

Testing Protocol Tips

- Think Aloud Protocol: Ask users to verbalize their thoughts
- No Leading: Don't guide them unless they're completely stuck
- Record: Screen recording + audio (with permission)
- Take Notes: One person facilitates, one person takes detailed notes
- Time Box: 10-20 minutes per session
- Thank Them: Small compensation (coffee gift card, etc.)

# 1. BACKGROUND
- What is your study line (name and BSc / MSc) and what semester are you on?
- How do you currently discover new courses to take?
- On a scale of 1-5, with 1 being easy, how difficult is it for you to find relevant courses?

# 2. Task-Based Testing (Observe & Measure)
- Please add 3 courses you've taken to the system using the search tab."
    - Time to complete / how long until they stopped browsing
    - Number of errors/corrections
    - Did they understand the search interface immediately?
- On a scale of 1-5, how easy was it to find and add courses? Why?
- Was the autocomplete helpful? Did people write course numbers or names? What could improve it?
- Did you notice the selected courses basket? Was it clear what it was for?

# 3, Bulk Entry Method
- Now try the 'Paste Course List' tab. Copy this text [provide sample grade sheet] and extract courses from it."
    - Time to complete / how long until they stopped browsing
    - Did they understand the purpose immediately?
- Was it clear there were two different ways to input courses?
- Which of the two methods do you prefer?
- Did the course recommendations match your expectations?
- Did it help you find courses that you A: Didn't already know about, and B: would consider taking?

# 4, Applying Filters
- Were the filter options clear and visible?
- Are there other filters you would want? (language, ECTS points, institute, schedule, etc.)

# 5, Course cards
- Is the course information displayed sufficient to make a decision?
- What information would you like to see? Is the description preview relevant and long enough?
- On a scale of 1-5, how visually appealing is the interface? Colors/styling etc.
- Is showing 10 results at a time appropriate, or would you prefer more/fewer? Is "Load More" button better than endless scroll?

# 6: Evaluating Quality
Instructions: "Look at the top 10 recommendations. Are they relevant to your interests?"
- Are there wrong or irrelevant recommendations? Why are they wrong?
- On a scale of 1-5, how relevant are these recommendations? (1=not at all, 5=very relevant)
- Are there courses you expected to see but didn't?

- Honestly, if I made this tool available on the internet, would you use it to plan your courses next semester?
- When would you use it?
    - When exploring possibilities
    - When filling my study plan
    - When I need to find an elective
- [OPTIONAL] Compare to my own website

# 7: Algorithm transparency and additional features
Did you consider the first 5 recommendations more than the next 5? After all, the algorithm ranked them higher.
Did you find the recommendations "trustworthy"? Do you care about it? Would any of these help:
- Explanations of why courses are similar
- Student reviews/ratings
- Success rates (pass/fail statistics)
- Prerequisites matching
On a scale of 1-5, how intelligent/insightful was the algorithm rankings?

# 8 Other stuff
- What would you change about the interface?
- Was anything confusing or unexpected?
- Any other comments or suggestions?

# FOR ME ONLY!
Quantitative Metrics to Track

Efficiency Metrics
- Average time to add 3 courses (search method)
- Average time to extract courses (bulk method)
- Average time to apply filters
- Time from submit to first interaction with results

Error Metrics
- Number of failed course searches
- Number of times users re-read instructions
- Number of times users backtracked or corrected actions

Engagement Metrics
Number of recommendations examined
- Percentage of users who load more results
- Percentage of users who try both input methods
- Percentage of users who apply filters
Satisfaction Scores (1-5 scale)

Overall satisfaction
- Ease of use
- Recommendation relevance
- Interface clarity
- Likelihood to use again

Red Flags to Watch For
During testing, note if users:
- Give up on a task
- Express confusion or frustration
- Misinterpret UI elements
- Ignore important features
- Expect functionality that doesn't exist
- Question the quality of recommendations
- Take much longer than expected
- Need to re-read instructions multiple times

