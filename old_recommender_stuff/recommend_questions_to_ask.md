*## 0. Testing Protocol Tips*
# I will not say anything / I will not guide you
# Please think aloud and verbalize your thoughts while you are in the middle of testing
# It'll take 5-10 minutes / We're audio recording (for note taking) and screen recording (for time measurements)

*## 1. BACKGROUND*
- Study line and semester
- How would you describe your general academic interests
- How do YOU find new courses?

*## 2. Task-Based Testing (Observe & Measure)*
# I want you to imagine that you need to find an elective course for next semester.
- With that in mind, use the tool in front of you.
    - *(Note: Time to complete / how long until they stopped browsing)*
    - *(Note: Did they understand the purpose immediately?)*
    - *(Note: Did people write course numbers or names?)*
    - *(Note: Number of errors/corrections)*
    - *(Note: Did they use filters? Which ones?)*
    - *(Note: % who load more results / % who try both input methods / % who apply filters)*
# Remember your goal of finding an elective for next semester.
    - Look at the top 10 recommendations. Are these recommendations helpful for you? Why/why not?
    - Was there some courses you expected to see, but didn't?
# Consider the two different ways of inputting courses (course search and external text)
    - Which did do you prefer and why? Overall, was the search interface intuitive and a good user experience to use?
        - Is submitting your courses as raw text a good solution? Or would you instead
        - Would you be interested in instead submitting an open-ended text prompt about yourself and your academic interests?
# Consider the two different ways to view the recommendations (endless list and caroussel)
    - Which did you prefer and why? Overall, was browsing the results intuitive and a good user experience to browse through?
    - Was the information displayed for each course sufficient? Would you like to see more/fewer courses?
    - How would you feel about multiple carousels with different filters/rankings, Netflix-style?
        - Do you think this would be better or worse than the endless one-dimensional scroll?
# There is a trade-off between how quickly you can scroll through courses, and the information shown for each course.
- Depth vs Breadth




*## 3 On a scale from 1-5, I think...*
# Answer the following question on a scale from 1-5
- ...The course recommendation algorithm was good at finding an elective course for my next semester.
    - ...Searching for courses was a good user experience.
    - ...Browsing the recommended courses was a good user experience.
    - ...The style and UI of the site is good.
# Now, let's ignore my tool for a moment.
    - ...DTU's existing tools are sufficient for helping me fill out my study plan.
    - ...Once this tool is finished, I could see myself using a tool such as this one to find courses.
    - ...In it's current implementation, I would use this tool to find courses.

## Final
# You were asked to imagine yourself needing to find an elective course for next semester.
If I changed the scenario, and you instead were a 1st year student needing to fill out your entire study plan...
Or if you were a 5th semester BSc student needing to fill out 45 ECTS points worth of electives...
    - Would this change how you perceived the usefulness of my tool?
    - In your opinion, who is this tool for?
- [OPTIONAL] Compare to my own website
- This project is about a Machine-learning powered course recommender.
    - To what extend did you care about the rankings provided by my algorithm?
    - Imagine that my site was instead a ChatGPT-like interface. How would you feel about an algorithm helping you find courses?






# FOR ME ONLY!
Quantitative Metrics to Track

Efficiency Metrics
- Average time to add 3 courses (search method)
- Average time to extract courses (bulk method)
- Average time to apply filters
- Time from submit to first interaction with results

Error Metrics
- Number of failed course searches
- Number of times users re-read instructions
- Number of times users backtracked or corrected actions

Engagement Metrics
Number of recommendations examined
- Percentage of users who load more results
- Percentage of users who try both input methods
- Percentage of users who apply filters
Satisfaction Scores (1-5 scale)

Overall satisfaction
- Ease of use
- Recommendation relevance
- Interface clarity
- Likelihood to use again

Red Flags to Watch For
During testing, note if users:
- Give up on a task
- Express confusion or frustration
- Misinterpret UI elements
- Ignore important features
- Expect functionality that doesn't exist
- Question the quality of recommendations
- Take much longer than expected
- Need to re-read instructions multiple times

